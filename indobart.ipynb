{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from src.datasets import IndoSum\n",
    "from src.common import get_device\n",
    "from src.indobart.base import get_model, get_tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 14262\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 3762\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indosum = IndoSum()\n",
    "indosum.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jakarta, CNN Indonesia - - Dokter Ryan Thamrin...</td>\n",
       "      <td>1501893029-lula-kamal-dokter-ryan-thamrin-saki...</td>\n",
       "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
       "      <td>1509072914-dua-smartphone-zenfone-baru-tawarka...</td>\n",
       "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jakarta, CNN Indonesia - - Dinas Pariwisata Pr...</td>\n",
       "      <td>1510613677-songsong-visit-2020-bengkulu-perkua...</td>\n",
       "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merdeka.com - Indonesia Corruption Watch (ICW)...</td>\n",
       "      <td>1502706803-icw-ada-kejanggalan-atas-tewasnya-s...</td>\n",
       "      <td>Indonesia Corruption Watch (ICW) meminta Komis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merdeka.com - Presiden Joko Widodo (Jokowi) me...</td>\n",
       "      <td>1503039338-pembagian-sepeda-usai-upacara-penur...</td>\n",
       "      <td>Jokowi memimpin upacara penurunan bendera. Usa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Jakarta, CNN Indonesia - - Dokter Ryan Thamrin...   \n",
       "1  Selfie ialah salah satu tema terpanas di kalan...   \n",
       "2  Jakarta, CNN Indonesia - - Dinas Pariwisata Pr...   \n",
       "3  Merdeka.com - Indonesia Corruption Watch (ICW)...   \n",
       "4  Merdeka.com - Presiden Joko Widodo (Jokowi) me...   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1501893029-lula-kamal-dokter-ryan-thamrin-saki...   \n",
       "1  1509072914-dua-smartphone-zenfone-baru-tawarka...   \n",
       "2  1510613677-songsong-visit-2020-bengkulu-perkua...   \n",
       "3  1502706803-icw-ada-kejanggalan-atas-tewasnya-s...   \n",
       "4  1503039338-pembagian-sepeda-usai-upacara-penur...   \n",
       "\n",
       "                                             summary  \n",
       "0  Dokter Lula Kamal yang merupakan selebriti sek...  \n",
       "1  Asus memperkenalkan   ZenFone generasi keempat...  \n",
       "2  Dinas Pariwisata Provinsi Bengkulu kembali men...  \n",
       "3  Indonesia Corruption Watch (ICW) meminta Komis...  \n",
       "4  Jokowi memimpin upacara penurunan bendera. Usa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indosum.to_pd(\"train\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(40004, 768, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(40004, 768, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MBartEncoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(40004, 768, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MBartDecoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=40004, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndoNLGTokenizer(name_or_path='indobenchmark/indobart-v2', vocab_size=40004, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<mask>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t40003: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"document\"], max_length=768, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result\n",
    "\n",
    "tokenized_ds = indosum.ds.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "def train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir + \"/checkpoint\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=generation_max_length,\n",
    "        log_level=\"info\",\n",
    "        logging_first_step=True,\n",
    "        logging_dir=output_dir + \"/logs\",\n",
    "        resume_from_checkpoint=True,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer\n",
    "    \n",
    "def evaluate_model(trainer):\n",
    "    eval_results = trainer.evaluate(eval_dataset=tokenized_ds[\"test\"])\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    trainer = train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length)\n",
    "    eval_results = evaluate_model(trainer)\n",
    "    \n",
    "    return trainer, eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation\n",
    "\n",
    "Try multiple generation max length with the rest parameters fixed.\n",
    "Observes the best score and the corresponding generation max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,262\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,349\n",
      "  Number of trainable parameters = 131,543,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5349/5349 23:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.517173</td>\n",
       "      <td>0.667031</td>\n",
       "      <td>0.597789</td>\n",
       "      <td>0.639028</td>\n",
       "      <td>0.659747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.502262</td>\n",
       "      <td>0.668084</td>\n",
       "      <td>0.597467</td>\n",
       "      <td>0.640838</td>\n",
       "      <td>0.660474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.509274</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.596807</td>\n",
       "      <td>0.640493</td>\n",
       "      <td>0.659551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Saving model checkpoint to ./results/00-indobart/01/checkpoint/checkpoint-1783\n",
      "/home/paperspace/thesis-binus/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-1783/config.json\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-1783/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/01/checkpoint/checkpoint-1783/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/01/checkpoint/checkpoint-1783/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/01/checkpoint/checkpoint-1783/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/01/checkpoint/checkpoint-3566\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-3566/config.json\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-3566/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/01/checkpoint/checkpoint-3566/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/01/checkpoint/checkpoint-3566/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/01/checkpoint/checkpoint-3566/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/01/checkpoint/checkpoint-1783] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/00-indobart/01/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/01/checkpoint/checkpoint-3566] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/01/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/01/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3762\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 12:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for experiment ===\n",
      "-- Params --\n",
      "{\n",
      "    \"output_dir\": \"./results/00-indobart/01\",\n",
      "    \"per_device_batch_size\": 8,\n",
      "    \"learning_rate\": 3.75e-05,\n",
      "    \"num_train_epochs\": 3,\n",
      "    \"generation_max_length\": 60\n",
      "}\n",
      "-- Eval results --\n",
      "{\n",
      "    \"eval_loss\": 0.5376449823379517,\n",
      "    \"eval_rouge1\": 0.6578535232670708,\n",
      "    \"eval_rouge2\": 0.5860617703657387,\n",
      "    \"eval_rougeL\": 0.6272031590547716,\n",
      "    \"eval_rougeLsum\": 0.6492784059277354,\n",
      "    \"eval_runtime\": 827.9129,\n",
      "    \"eval_samples_per_second\": 4.544,\n",
      "    \"eval_steps_per_second\": 0.569,\n",
      "    \"epoch\": 3.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,262\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,349\n",
      "  Number of trainable parameters = 131,543,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5349/5349 24:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.556184</td>\n",
       "      <td>0.683701</td>\n",
       "      <td>0.606891</td>\n",
       "      <td>0.652541</td>\n",
       "      <td>0.675584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.556206</td>\n",
       "      <td>0.690680</td>\n",
       "      <td>0.613901</td>\n",
       "      <td>0.660262</td>\n",
       "      <td>0.682306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.566920</td>\n",
       "      <td>0.691895</td>\n",
       "      <td>0.616090</td>\n",
       "      <td>0.661770</td>\n",
       "      <td>0.683992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/02/checkpoint/checkpoint-1783\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-1783/config.json\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-1783/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/02/checkpoint/checkpoint-1783/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/02/checkpoint/checkpoint-1783/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/02/checkpoint/checkpoint-1783/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/02/checkpoint/checkpoint-3566\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-3566/config.json\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-3566/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/02/checkpoint/checkpoint-3566/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/02/checkpoint/checkpoint-3566/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/02/checkpoint/checkpoint-3566/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/02/checkpoint/checkpoint-1783] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/00-indobart/02/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/02/checkpoint/checkpoint-3566] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/02/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/02/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3762\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 14:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for experiment ===\n",
      "-- Params --\n",
      "{\n",
      "    \"output_dir\": \"./results/00-indobart/02\",\n",
      "    \"per_device_batch_size\": 8,\n",
      "    \"learning_rate\": 3.75e-05,\n",
      "    \"num_train_epochs\": 3,\n",
      "    \"generation_max_length\": 70\n",
      "}\n",
      "-- Eval results --\n",
      "{\n",
      "    \"eval_loss\": 0.5976317524909973,\n",
      "    \"eval_rouge1\": 0.682377584319096,\n",
      "    \"eval_rouge2\": 0.6055809609561018,\n",
      "    \"eval_rougeL\": 0.6490064606746873,\n",
      "    \"eval_rougeLsum\": 0.6728720102402448,\n",
      "    \"eval_runtime\": 899.077,\n",
      "    \"eval_samples_per_second\": 4.184,\n",
      "    \"eval_steps_per_second\": 0.524,\n",
      "    \"epoch\": 3.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,262\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,349\n",
      "  Number of trainable parameters = 131,543,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5349/5349 26:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.638636</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>0.646978</td>\n",
       "      <td>0.673668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.644547</td>\n",
       "      <td>0.689555</td>\n",
       "      <td>0.610778</td>\n",
       "      <td>0.656815</td>\n",
       "      <td>0.680999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.639322</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.603201</td>\n",
       "      <td>0.650882</td>\n",
       "      <td>0.675437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/03/checkpoint/checkpoint-1783\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-1783/config.json\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-1783/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/03/checkpoint/checkpoint-1783/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/03/checkpoint/checkpoint-1783/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/03/checkpoint/checkpoint-1783/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/03/checkpoint/checkpoint-3566\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-3566/config.json\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-3566/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/03/checkpoint/checkpoint-3566/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/03/checkpoint/checkpoint-3566/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/03/checkpoint/checkpoint-3566/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/03/checkpoint/checkpoint-1783] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/00-indobart/03/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/03/checkpoint/checkpoint-3566] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/03/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/03/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3762\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 17:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for experiment ===\n",
      "-- Params --\n",
      "{\n",
      "    \"output_dir\": \"./results/00-indobart/03\",\n",
      "    \"per_device_batch_size\": 8,\n",
      "    \"learning_rate\": 3.75e-05,\n",
      "    \"num_train_epochs\": 3,\n",
      "    \"generation_max_length\": 80\n",
      "}\n",
      "-- Eval results --\n",
      "{\n",
      "    \"eval_loss\": 0.6712408065795898,\n",
      "    \"eval_rouge1\": 0.6813039421192061,\n",
      "    \"eval_rouge2\": 0.600212172181025,\n",
      "    \"eval_rougeL\": 0.6452167875185628,\n",
      "    \"eval_rougeLsum\": 0.6714865512581554,\n",
      "    \"eval_runtime\": 1106.0189,\n",
      "    \"eval_samples_per_second\": 3.401,\n",
      "    \"eval_steps_per_second\": 0.426,\n",
      "    \"epoch\": 3.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,262\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,349\n",
      "  Number of trainable parameters = 131,543,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5349/5349 28:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.734305</td>\n",
       "      <td>0.664274</td>\n",
       "      <td>0.581619</td>\n",
       "      <td>0.628552</td>\n",
       "      <td>0.655224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.735515</td>\n",
       "      <td>0.675261</td>\n",
       "      <td>0.594191</td>\n",
       "      <td>0.640338</td>\n",
       "      <td>0.667221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.699859</td>\n",
       "      <td>0.673045</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.637685</td>\n",
       "      <td>0.664565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/04/checkpoint/checkpoint-1783\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-1783/config.json\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-1783/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/04/checkpoint/checkpoint-1783/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/04/checkpoint/checkpoint-1783/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/04/checkpoint/checkpoint-1783/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/04/checkpoint/checkpoint-3566\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-3566/config.json\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-3566/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/04/checkpoint/checkpoint-3566/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/04/checkpoint/checkpoint-3566/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/04/checkpoint/checkpoint-3566/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/04/checkpoint/checkpoint-1783] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/00-indobart/04/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/04/checkpoint/checkpoint-3566] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/04/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/04/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3762\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 19:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for experiment ===\n",
      "-- Params --\n",
      "{\n",
      "    \"output_dir\": \"./results/00-indobart/04\",\n",
      "    \"per_device_batch_size\": 8,\n",
      "    \"learning_rate\": 3.75e-05,\n",
      "    \"num_train_epochs\": 3,\n",
      "    \"generation_max_length\": 90\n",
      "}\n",
      "-- Eval results --\n",
      "{\n",
      "    \"eval_loss\": 0.7304822206497192,\n",
      "    \"eval_rouge1\": 0.671626285419908,\n",
      "    \"eval_rouge2\": 0.5889033857964887,\n",
      "    \"eval_rougeL\": 0.6343955226105362,\n",
      "    \"eval_rougeLsum\": 0.6622529154483132,\n",
      "    \"eval_runtime\": 1259.7242,\n",
      "    \"eval_samples_per_second\": 2.986,\n",
      "    \"eval_steps_per_second\": 0.374,\n",
      "    \"epoch\": 3.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,262\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,349\n",
      "  Number of trainable parameters = 131,543,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5349/5349 28:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.793746</td>\n",
       "      <td>0.655516</td>\n",
       "      <td>0.573237</td>\n",
       "      <td>0.619078</td>\n",
       "      <td>0.647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.772315</td>\n",
       "      <td>0.658569</td>\n",
       "      <td>0.576196</td>\n",
       "      <td>0.621591</td>\n",
       "      <td>0.649946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.744207</td>\n",
       "      <td>0.656982</td>\n",
       "      <td>0.574017</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.648578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/05/checkpoint/checkpoint-1783\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-1783/config.json\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-1783/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/05/checkpoint/checkpoint-1783/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/05/checkpoint/checkpoint-1783/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/05/checkpoint/checkpoint-1783/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/05/checkpoint/checkpoint-3566\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-3566/config.json\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-3566/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/05/checkpoint/checkpoint-3566/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/05/checkpoint/checkpoint-3566/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/05/checkpoint/checkpoint-3566/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/05/checkpoint/checkpoint-1783] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/00-indobart/05/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "Deleting older checkpoint [results/00-indobart/05/checkpoint/checkpoint-3566] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/00-indobart/05/checkpoint/checkpoint-5349\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/config.json\n",
      "Configuration saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/generation_config.json\n",
      "Model weights saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/model.safetensors\n",
      "tokenizer config file saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/tokenizer_config.json\n",
      "Special tokens file saved in ./results/00-indobart/05/checkpoint/checkpoint-5349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3762\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 20:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for experiment ===\n",
      "-- Params --\n",
      "{\n",
      "    \"output_dir\": \"./results/00-indobart/05\",\n",
      "    \"per_device_batch_size\": 8,\n",
      "    \"learning_rate\": 3.75e-05,\n",
      "    \"num_train_epochs\": 3,\n",
      "    \"generation_max_length\": 100\n",
      "}\n",
      "-- Eval results --\n",
      "{\n",
      "    \"eval_loss\": 0.7771861553192139,\n",
      "    \"eval_rouge1\": 0.6505126244879144,\n",
      "    \"eval_rouge2\": 0.5673431688996508,\n",
      "    \"eval_rougeL\": 0.6124387532072134,\n",
      "    \"eval_rougeLsum\": 0.641838960450171,\n",
      "    \"eval_runtime\": 1295.8065,\n",
      "    \"eval_samples_per_second\": 2.903,\n",
      "    \"eval_steps_per_second\": 0.363,\n",
      "    \"epoch\": 3.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "experiments = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    generation_max_length = 50 + i * 10\n",
    "    experiments.append({\n",
    "        \"output_dir\": f\"./results/00-indobart/0{i}\",\n",
    "        \"per_device_batch_size\": 8,\n",
    "        \"learning_rate\": 3.75e-5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"generation_max_length\": generation_max_length\n",
    "    })\n",
    "\n",
    "for exp in experiments:\n",
    "    os.makedirs(exp[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    trainer, eval_results = train_and_evaluate(\n",
    "        exp[\"output_dir\"],\n",
    "        exp[\"per_device_batch_size\"],\n",
    "        exp[\"learning_rate\"],\n",
    "        exp[\"num_train_epochs\"],\n",
    "        exp[\"generation_max_length\"]\n",
    "    )\n",
    "    \n",
    "    # print params and the results\n",
    "    print(\"=== Results for experiment ===\")\n",
    "    print(\"-- Params --\") \n",
    "    print(json.dumps(exp, indent=4))\n",
    "    print(\"-- Eval results --\")\n",
    "    print(json.dumps(eval_results, indent=4))\n",
    "    \n",
    "    # save mapping between params and results\n",
    "    with open(exp[\"output_dir\"] + \"/params.json\", \"w\") as f:\n",
    "        json.dump(exp, f)\n",
    "    \n",
    "    with open(exp[\"output_dir\"] + \"/eval_results.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
