{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasets import IndoSum\n",
    "from src.common import get_device\n",
    "from src.indobart.base import get_model, get_tokenizer, get_config\n",
    "\n",
    "import stanza\n",
    "import torch\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy import displacy\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from transformers import BartModel, BartConfig\n",
    "from transformers import MBartForConditionalGeneration, MBartConfig\n",
    "from transformers.models.bart.modeling_bart import BartAttention\n",
    "import torch.nn as nn\n",
    "\n",
    "import diskcache as dc\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up Stanza's Indonesian NLP model\n",
    "stanza.download(\"id\")\n",
    "nlp = stanza.Pipeline(\"id\", processors=\"tokenize,pos,lemma,depparse\", use_gpu=True, device=device)\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum = IndoSum()\n",
    "indosum.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum.to_pd(\"train\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Stanza Document to spaCy Doc for Visualization\n",
    "def stanza_to_spacy(doc):\n",
    "    \"\"\"\n",
    "    Converts a Stanza-parsed document to a spaCy Doc object for dependency visualization.\n",
    "    Handles root words and ensures valid head indices.\n",
    "    \"\"\"\n",
    "    # Flatten all sentences' words, heads, and dependency relations from Stanza\n",
    "    words = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "    deps = [word.deprel for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "    # Stanza `head` is 1-based (1-indexed); convert to 0-based for spaCy and handle roots\n",
    "    heads = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            # If the word is root, set head to its own index\n",
    "            if word.head == 0:\n",
    "                heads.append(word.id - 1)\n",
    "            else:\n",
    "                heads.append(word.head - 1)  # Convert to 0-based indexing\n",
    "\n",
    "    # Create a spaCy Doc object using the extracted information\n",
    "    spacy_doc = Doc(spacy.blank(\"id\").vocab, words=words)\n",
    "    for token, head, dep in zip(spacy_doc, heads, deps):\n",
    "        token.dep_ = dep\n",
    "        token.head = spacy_doc[head]\n",
    "\n",
    "    return spacy_doc\n",
    "\n",
    "# Build Dependency Information Matrices (DIM) for each sentence in a document\n",
    "def build_dependency_matrices(document):\n",
    "    \"\"\"\n",
    "    Build a list of Dependency Information Matrices (DIMs) for each sentence in the document.\n",
    "    Each matrix represents dependency relations within a sentence.\n",
    "    \"\"\"\n",
    "    doc = nlp(document)  # Process the document with Stanza\n",
    "    matrices = []  # List to hold the DIM for each sentence in the document\n",
    "    sentence_texts = []  # List to hold the raw sentences\n",
    "\n",
    "    # Iterate over each sentence in the processed Stanza document\n",
    "    for sentence in doc.sentences:\n",
    "        n = len(sentence.words)  # Number of words in the sentence\n",
    "        matrix = np.zeros((n, n))  # Initialize an n x n matrix with zeros\n",
    "\n",
    "        # Populate the matrix with dependency information\n",
    "        for word in sentence.words:\n",
    "            if word.head > 0:  # If head is not root (head == 0 indicates root in Stanza)\n",
    "                # Set a 1 for both directions (i.e., word -> head and head -> word)\n",
    "                matrix[word.id - 1, word.head - 1] = 1  # word.id and word.head are 1-based indices\n",
    "                matrix[word.head - 1, word.id - 1] = 1  # Make the matrix symmetric\n",
    "        matrices.append(torch.tensor(matrix, dtype=torch.float32))  # Convert matrix to tensor and add to list\n",
    "        sentence_texts.append(sentence.text)  # Add the raw sentence text to the list\n",
    "\n",
    "    return list(zip(matrices, sentence_texts)), doc\n",
    "\n",
    "# Parse and Visualize Dependencies\n",
    "def visualize_dependencies(doc):\n",
    "    \"\"\"\n",
    "    Visualizes dependencies from the Stanza-parsed document using spaCy's displacy.\n",
    "    \"\"\"\n",
    "    # Convert Stanza output to spaCy format for visualization\n",
    "    spacy_doc = stanza_to_spacy(doc)\n",
    "    \n",
    "    # Visualize dependencies using spaCy's displacy\n",
    "    displacy.render(spacy_doc, style=\"dep\", jupyter=True)  # Use jupyter=True in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = indosum.ds[\"validation\"][0]['document']\n",
    "sample_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dim_sentence_pairs, sample_stanza_doc = build_dependency_matrices(sample_doc)\n",
    "print(\"Dependency Information Matrices for each sentence:\")\n",
    "for i, (matrix, sentence) in enumerate(sample_dim_sentence_pairs, 1):\n",
    "    print(f\"Raw Sentence {i}:\\n{sentence}\")\n",
    "    print(f\"DIM Sentence {i}:\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic-Guided Attention in the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom attention layer for the encoder to use the DIM during the attention calculation\n",
    "class EncoderLinguisticGuidedAttention(BartAttention):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, alpha=1.0):\n",
    "        super().__init__(embed_dim, num_heads, dropout, is_decoder=False)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, hidden_states, dim_matrix, **kwargs):\n",
    "        attn_output, attn_weights = super().forward(hidden_states, **kwargs)\n",
    "        \n",
    "        # Apply linguistic-guided attention\n",
    "        dim_matrix = dim_matrix.to(attn_weights.device)\n",
    "        lg_attn_weights = (self.alpha * dim_matrix + torch.eye(dim_matrix.size(-1), device=dim_matrix.device)) * attn_weights\n",
    "        attn_output = torch.matmul(lg_attn_weights, hidden_states)\n",
    "        return attn_output, lg_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, replace the encoder's attention mechanism with EncoderLinguisticGuidedAttention.\n",
    "class CustomIndoMBartWithLGA(MBartForConditionalGeneration):\n",
    "    def __init__(self, config: MBartConfig, alpha=1.0):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Modify encoder layers to use linguistic-guided attention\n",
    "        for layer in self.model.encoder.layers:\n",
    "            layer.self_attn = EncoderLinguisticGuidedAttention(\n",
    "                config.d_model, config.encoder_attention_heads, config.attention_dropout, alpha=alpha\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomIndoMBartWithLGA(config, alpha=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(get_model().state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data collator to include DIM\n",
    "class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        # Flatten and pad DIMs across documents in the batch for consistent dimensions\n",
    "        max_sentences = max(len(f[\"dim_matrices\"]) for f in features)\n",
    "        max_tokens = max(matrix.size(0) for f in features for matrix in f[\"dim_matrices\"])\n",
    "\n",
    "        # Initialize padded tensor for batched DIMs\n",
    "        dim_matrices_padded = torch.zeros((len(features), max_sentences, max_tokens, max_tokens))\n",
    "\n",
    "        for i, feature in enumerate(features):\n",
    "            for j, matrix in enumerate(feature[\"dim_matrices\"]):\n",
    "                dim_matrices_padded[i, j, :matrix.size(0), :matrix.size(1)] = matrix\n",
    "\n",
    "        batch[\"dim_matrices\"] = dim_matrices_padded\n",
    "        return batch\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the cache\n",
    "os.makedirs(f\"./results/00-indobart-dp/dim_cache\", exist_ok=True)\n",
    "cache_path = \"./results/00-indobart-dp/dim_cache/dim_cache.pkl\"\n",
    "\n",
    "# Dictionary to hold all precomputed DIMs\n",
    "dim_cache = {}\n",
    "\n",
    "# Precompute DIMs and store them in the dictionary with progress tracking\n",
    "def precompute_dims(documents):\n",
    "    print(\"Precomputing Dependency Information Matrices (DIMs) for documents...\")\n",
    "    for i, document in enumerate(tqdm(documents, desc=\"Computing DIMs\")):\n",
    "        dim_sentence_pairs, _ = build_dependency_matrices(document)\n",
    "        dim_matrices, _ = zip(*dim_sentence_pairs)\n",
    "        dim_cache[i] = [dm.tolist() for dm in dim_matrices]  # Store DIMs as lists for easier serialization\n",
    "\n",
    "# Save the entire DIM cache as a single file with progress tracking\n",
    "def save_cache(filename=\"dim_cache.pkl\"):\n",
    "    print(f\"Saving DIM cache to {filename}...\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(dim_cache, f)\n",
    "    print(\"DIM cache saved successfully.\")\n",
    "\n",
    "# Load the DIM cache from a single file\n",
    "def load_cache(filename=\"dim_cache.pkl\"):\n",
    "    print(f\"Loading DIM cache from {filename}...\")\n",
    "    with open(filename, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    print(\"DIM cache loaded successfully.\")\n",
    "    return cache\n",
    "\n",
    "# Main function to check cache existence, load or precompute DIMs, and save if necessary\n",
    "def prepare_dim_cache(documents, cache_path=\"dim_cache.pkl\"):\n",
    "    global dim_cache  # Use the global dim_cache dictionary\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        # If cache file exists, load it\n",
    "        dim_cache = load_cache(cache_path)\n",
    "    else:\n",
    "        # If cache file does not exist, precompute DIMs and save to cache\n",
    "        precompute_dims(documents)\n",
    "        save_cache(cache_path)\n",
    "\n",
    "# Example usage\n",
    "documents = indosum.ds[\"train\"][\"document\"]  # Replace with your dataset's document list\n",
    "prepare_dim_cache(documents, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"document\"], max_length=768, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Retrieve DIMs from the loaded cache for each document in the batch\n",
    "    dim_matrices_batch = []\n",
    "    for i, document in enumerate(examples[\"document\"]):\n",
    "        if i in dim_cache:\n",
    "            # Convert the cached DIMs (stored as lists) back to tensors\n",
    "            dim_matrices = [torch.tensor(dm) for dm in dim_cache[i]]\n",
    "        else:\n",
    "            # If a documentâ€™s DIMs are not in the cache, handle it (optional)\n",
    "            dim_sentence_pairs, _ = build_dependency_matrices(document)\n",
    "            dim_matrices, _ = zip(*dim_sentence_pairs)\n",
    "            dim_matrices = [torch.tensor(dm) for dm in dim_matrices]\n",
    "            dim_cache[i] = [dm.tolist() for dm in dim_matrices]  # Add to cache for future use\n",
    "\n",
    "        dim_matrices_batch.append(dim_matrices)\n",
    "\n",
    "    model_inputs[\"dim_matrices\"] = dim_matrices_batch\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = indosum.ds.map(preprocess_function, batched=True)\n",
    "\n",
    "def train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir + \"/checkpoint\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=generation_max_length,\n",
    "        log_level=\"info\",\n",
    "        logging_first_step=True,\n",
    "        logging_dir=output_dir + \"/logs\",\n",
    "        resume_from_checkpoint=True,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer\n",
    "    \n",
    "def evaluate_model(trainer):\n",
    "    eval_results = trainer.evaluate(eval_dataset=tokenized_ds[\"test\"])\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    trainer = train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length)\n",
    "    eval_results = evaluate_model(trainer)\n",
    "    \n",
    "    return trainer, eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation\n",
    "\n",
    "Try multiple generation max length with the rest parameters fixed.\n",
    "Observes the best score and the corresponding generation max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    generation_max_length = 50 + i * 10\n",
    "    experiments.append({\n",
    "        \"output_dir\": f\"./results/00-indobart-dp/model/0{i}\",\n",
    "        \"per_device_batch_size\": 8,\n",
    "        \"learning_rate\": 3.75e-5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"generation_max_length\": generation_max_length\n",
    "    })\n",
    "\n",
    "for exp in experiments:\n",
    "    os.makedirs(exp[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    trainer, eval_results = train_and_evaluate(\n",
    "        exp[\"output_dir\"],\n",
    "        exp[\"per_device_batch_size\"],\n",
    "        exp[\"learning_rate\"],\n",
    "        exp[\"num_train_epochs\"],\n",
    "        exp[\"generation_max_length\"]\n",
    "    )\n",
    "    \n",
    "    # print params and the results\n",
    "    print(\"=== Results for experiment ===\")\n",
    "    print(\"-- Params --\") \n",
    "    print(json.dumps(exp, indent=4))\n",
    "    print(\"-- Eval results --\")\n",
    "    print(json.dumps(eval_results, indent=4))\n",
    "    \n",
    "    # save mapping between params and results\n",
    "    with open(exp[\"output_dir\"] + \"/params.json\", \"w\") as f:\n",
    "        json.dump(exp, f)\n",
    "    \n",
    "    with open(exp[\"output_dir\"] + \"/eval_results.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
