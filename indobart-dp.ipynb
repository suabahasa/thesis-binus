{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasets import IndoSum\n",
    "from src.common import get_device\n",
    "from src.indobart.base import get_model, get_tokenizer, get_config\n",
    "\n",
    "import stanza\n",
    "import torch\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy import displacy\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from transformers import BartModel, BartConfig\n",
    "from transformers import MBartForConditionalGeneration, MBartConfig\n",
    "from transformers.models.bart.modeling_bart import BartAttention\n",
    "import torch.nn as nn\n",
    "\n",
    "import diskcache as dc\n",
    "import pickle\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc24fa3164343a89fcda40ebe2a6ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 19:24:38 INFO: Downloaded file to /home/paperspace/stanza_resources/resources.json\n",
      "2024-11-14 19:24:38 INFO: Downloading default packages for language: id (Indonesian) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 19:24:39 INFO: File exists: /home/paperspace/stanza_resources/id/default.zip\n",
      "2024-11-14 19:24:47 INFO: Finished downloading models and saved to /home/paperspace/stanza_resources\n",
      "2024-11-14 19:24:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e4c9cf27dc4f8784a31f18683245ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 19:24:47 INFO: Downloaded file to /home/paperspace/stanza_resources/resources.json\n",
      "2024-11-14 19:24:47 WARNING: Language id package default expects mwt, which has been added\n",
      "2024-11-14 19:24:48 INFO: Loading these models for language: id (Indonesian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd_charlm   |\n",
      "| lemma     | gsd_nocharlm |\n",
      "| depparse  | gsd_charlm   |\n",
      "============================\n",
      "\n",
      "2024-11-14 19:24:48 INFO: Using device: cuda\n",
      "2024-11-14 19:24:48 INFO: Loading: tokenize\n",
      "2024-11-14 19:24:48 INFO: Loading: mwt\n",
      "2024-11-14 19:24:48 INFO: Loading: pos\n",
      "/home/paperspace/thesis-binus/.venv/lib/python3.9/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/paperspace/thesis-binus/.venv/lib/python3.9/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/paperspace/thesis-binus/.venv/lib/python3.9/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-14 19:24:48 INFO: Loading: lemma\n",
      "2024-11-14 19:24:49 INFO: Loading: depparse\n",
      "/home/paperspace/thesis-binus/.venv/lib/python3.9/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-14 19:24:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Download and set up Stanza's Indonesian NLP model\n",
    "stanza.download(\"id\")\n",
    "nlp = stanza.Pipeline(\"id\", processors=\"tokenize,pos,lemma,depparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum = IndoSum()\n",
    "indosum.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum.to_pd(\"train\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Stanza Document to spaCy Doc for Visualization\n",
    "def stanza_to_spacy(doc):\n",
    "    \"\"\"\n",
    "    Converts a Stanza-parsed document to a spaCy Doc object for dependency visualization.\n",
    "    Handles root words and ensures valid head indices.\n",
    "    \"\"\"\n",
    "    # Flatten all sentences' words, heads, and dependency relations from Stanza\n",
    "    words = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "    deps = [word.deprel for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "    # Stanza `head` is 1-based (1-indexed); convert to 0-based for spaCy and handle roots\n",
    "    heads = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            # If the word is root, set head to its own index\n",
    "            if word.head == 0:\n",
    "                heads.append(word.id - 1)\n",
    "            else:\n",
    "                heads.append(word.head - 1)  # Convert to 0-based indexing\n",
    "\n",
    "    # Create a spaCy Doc object using the extracted information\n",
    "    spacy_doc = Doc(spacy.blank(\"id\").vocab, words=words)\n",
    "    for token, head, dep in zip(spacy_doc, heads, deps):\n",
    "        token.dep_ = dep\n",
    "        token.head = spacy_doc[head]\n",
    "\n",
    "    return spacy_doc\n",
    "\n",
    "# Build Dependency Information Matrices (DIM) for each sentence in a document\n",
    "def build_dependency_matrices(document):\n",
    "    \"\"\"\n",
    "    Build a list of Dependency Information Matrices (DIMs) for each sentence in the document.\n",
    "    Each matrix represents dependency relations within a sentence.\n",
    "    \"\"\"\n",
    "    doc = nlp(document)  # Process the document with Stanza\n",
    "    matrices = []  # List to hold the DIM for each sentence in the document\n",
    "    sentence_texts = []  # List to hold the raw sentences\n",
    "\n",
    "    # Iterate over each sentence in the processed Stanza document\n",
    "    for sentence in doc.sentences:\n",
    "        n = len(sentence.words)  # Number of words in the sentence\n",
    "        matrix = np.zeros((n, n))  # Initialize an n x n matrix with zeros\n",
    "\n",
    "        # Populate the matrix with dependency information\n",
    "        for word in sentence.words:\n",
    "            if word.head > 0:  # If head is not root (head == 0 indicates root in Stanza)\n",
    "                # Set a 1 for both directions (i.e., word -> head and head -> word)\n",
    "                matrix[word.id - 1, word.head - 1] = 1  # word.id and word.head are 1-based indices\n",
    "                matrix[word.head - 1, word.id - 1] = 1  # Make the matrix symmetric\n",
    "        matrices.append(torch.tensor(matrix, dtype=torch.float32))  # Convert matrix to tensor and add to list\n",
    "        sentence_texts.append(sentence.text)  # Add the raw sentence text to the list\n",
    "\n",
    "    return list(zip(matrices, sentence_texts)), doc\n",
    "\n",
    "# Parse and Visualize Dependencies\n",
    "def visualize_dependencies(doc):\n",
    "    \"\"\"\n",
    "    Visualizes dependencies from the Stanza-parsed document using spaCy's displacy.\n",
    "    \"\"\"\n",
    "    # Convert Stanza output to spaCy format for visualization\n",
    "    spacy_doc = stanza_to_spacy(doc)\n",
    "    \n",
    "    # Visualize dependencies using spaCy's displacy\n",
    "    displacy.render(spacy_doc, style=\"dep\", jupyter=True)  # Use jupyter=True in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = indosum.ds[\"validation\"][0]['document']\n",
    "sample_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dim_sentence_pairs, sample_stanza_doc = build_dependency_matrices(sample_doc)\n",
    "print(\"Dependency Information Matrices for each sentence:\")\n",
    "for i, (matrix, sentence) in enumerate(sample_dim_sentence_pairs, 1):\n",
    "    print(f\"Raw Sentence {i}:\\n{sentence}\")\n",
    "    print(f\"DIM Sentence {i}:\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic-Guided Attention in the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom attention layer for the encoder to use the DIM during the attention calculation\n",
    "class EncoderLinguisticGuidedAttention(BartAttention):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, alpha=1.0):\n",
    "        super().__init__(embed_dim, num_heads, dropout, is_decoder=False)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, hidden_states, dim_matrix, **kwargs):\n",
    "        attn_output, attn_weights = super().forward(hidden_states, **kwargs)\n",
    "        \n",
    "        # Apply linguistic-guided attention\n",
    "        dim_matrix = dim_matrix.to(attn_weights.device)\n",
    "        lg_attn_weights = (self.alpha * dim_matrix + torch.eye(dim_matrix.size(-1), device=dim_matrix.device)) * attn_weights\n",
    "        attn_output = torch.matmul(lg_attn_weights, hidden_states)\n",
    "        return attn_output, lg_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, replace the encoder's attention mechanism with EncoderLinguisticGuidedAttention.\n",
    "class CustomIndoMBartWithLGA(MBartForConditionalGeneration):\n",
    "    def __init__(self, config: MBartConfig, alpha=1.0):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Modify encoder layers to use linguistic-guided attention\n",
    "        for layer in self.model.encoder.layers:\n",
    "            layer.self_attn = EncoderLinguisticGuidedAttention(\n",
    "                config.d_model, config.encoder_attention_heads, config.attention_dropout, alpha=alpha\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomIndoMBartWithLGA(config, alpha=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(get_model().state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data collator to include DIM\n",
    "class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        # Flatten and pad DIMs across documents in the batch for consistent dimensions\n",
    "        max_sentences = max(len(f[\"dim_matrices\"]) for f in features)\n",
    "        max_tokens = max(matrix.size(0) for f in features for matrix in f[\"dim_matrices\"])\n",
    "\n",
    "        # Initialize padded tensor for batched DIMs\n",
    "        dim_matrices_padded = torch.zeros((len(features), max_sentences, max_tokens, max_tokens))\n",
    "\n",
    "        for i, feature in enumerate(features):\n",
    "            for j, matrix in enumerate(feature[\"dim_matrices\"]):\n",
    "                dim_matrices_padded[i, j, :matrix.size(0), :matrix.size(1)] = matrix\n",
    "\n",
    "        batch[\"dim_matrices\"] = dim_matrices_padded\n",
    "        return batch\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the cache\n",
    "os.makedirs(f\"./results/00-indobart-dp/dim_cache\", exist_ok=True)\n",
    "\n",
    "# Dictionary to hold all precomputed DIMs\n",
    "cache_path = \"./results/00-indobart-dp/dim_cache/dim_cache.pkl\"\n",
    "dim_cache = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Precompute DIMs and store them in the dictionary with progress tracking\n",
    "def precompute_dims(documents):\n",
    "    print(\"Precomputing Dependency Information Matrices (DIMs) for documents...\")\n",
    "    for i, document in enumerate(tqdm(documents, desc=\"Computing DIMs\")):\n",
    "        dim_sentence_pairs, _ = build_dependency_matrices(document)\n",
    "        dim_matrices, _ = zip(*dim_sentence_pairs)\n",
    "        dim_cache[i] = [dm.tolist() for dm in dim_matrices]  # Store DIMs as lists for easier serialization\n",
    "\n",
    "# Save the entire DIM cache as a single file with progress tracking\n",
    "def save_cache(filename=cache_path):\n",
    "    print(f\"Saving DIM cache to {filename}...\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(dim_cache, f)\n",
    "    print(\"DIM cache saved successfully.\")\n",
    "\n",
    "# Load the DIM cache from a single file\n",
    "def load_cache(filename=cache_path):\n",
    "    print(f\"Loading DIM cache from {filename}...\")\n",
    "    with open(filename, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    print(\"DIM cache loaded successfully.\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Precompute DIMs and store them in the dictionary\n",
    "# def precompute_dims(documents):\n",
    "#     for i, document in enumerate(documents):\n",
    "#         dim_sentence_pairs, _ = build_dependency_matrices(document)\n",
    "#         dim_matrices, _ = zip(*dim_sentence_pairs)\n",
    "#         dim_cache[i] = [dm.tolist() for dm in dim_matrices]  # Store DIMs as lists for easier serialization\n",
    "\n",
    "# # Save the entire DIM cache as a single file\n",
    "# def save_cache(filename=cache_path):\n",
    "#     with open(filename, \"wb\") as f:\n",
    "#         pickle.dump(dim_cache, f)\n",
    "\n",
    "# # Load the DIM cache from a single file\n",
    "# def load_cache(filename=cache_path):\n",
    "#     with open(filename, \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# Example usage: Precompute DIMs and save to file\n",
    "# documents = indosum.ds[\"train\"][\"document\"]  # Replace with your dataset's document list\n",
    "precompute_dims(indosum.ds[\"train\"][\"document\"])\n",
    "save_cache(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"document\"], max_length=768, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Process DIMs for each document\n",
    "    # Process each document separately to build DIMs\n",
    "    # dim_matrices_batch = []\n",
    "    # for document in examples[\"document\"]:\n",
    "    #     dim_sentence_pairs, _ = build_dependency_matrices(document)  # Process each document\n",
    "    #     dim_matrices, _ = zip(*dim_sentence_pairs)  # Separate DIMs from text\n",
    "    #     dim_matrices_batch.append(list(dim_matrices))  # Collect all DIMs for the document\n",
    "\n",
    "    # model_inputs[\"dim_matrices\"] = dim_matrices_batch\n",
    "    \n",
    "    # Retrieve DIMs from cache for each document in the batch\n",
    "    dim_matrices_batch = []\n",
    "    for i, document in enumerate(examples[\"document\"]):\n",
    "        if i in cache:\n",
    "            dim_matrices = cache[i]  # Retrieve from cache\n",
    "        else:\n",
    "            # Compute and cache DIMs if not already cached\n",
    "            dim_sentence_pairs, _ = build_dependency_matrices(document)\n",
    "            dim_matrices, _ = zip(*dim_sentence_pairs)\n",
    "            dim_matrices = [dm.tolist() for dm in dim_matrices]\n",
    "            cache[i] = dim_matrices  # Store in cache\n",
    "\n",
    "        dim_matrices_batch.append([torch.tensor(dm) for dm in dim_matrices])  # Convert back to tensor\n",
    "\n",
    "    model_inputs[\"dim_matrices\"] = dim_matrices_batch\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = indosum.ds.map(preprocess_function, batched=True)\n",
    "\n",
    "def train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir + \"/checkpoint\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=generation_max_length,\n",
    "        log_level=\"info\",\n",
    "        logging_first_step=True,\n",
    "        logging_dir=output_dir + \"/logs\",\n",
    "        resume_from_checkpoint=True,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer\n",
    "    \n",
    "def evaluate_model(trainer):\n",
    "    eval_results = trainer.evaluate(eval_dataset=tokenized_ds[\"test\"])\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length):\n",
    "    trainer = train_model(output_dir, per_device_batch_size, learning_rate, num_train_epochs, generation_max_length)\n",
    "    eval_results = evaluate_model(trainer)\n",
    "    \n",
    "    return trainer, eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation\n",
    "\n",
    "Try multiple generation max length with the rest parameters fixed.\n",
    "Observes the best score and the corresponding generation max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    generation_max_length = 50 + i * 10\n",
    "    experiments.append({\n",
    "        \"output_dir\": f\"./results/00-indobart-dp/model/0{i}\",\n",
    "        \"per_device_batch_size\": 8,\n",
    "        \"learning_rate\": 3.75e-5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"generation_max_length\": generation_max_length\n",
    "    })\n",
    "\n",
    "for exp in experiments:\n",
    "    os.makedirs(exp[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    trainer, eval_results = train_and_evaluate(\n",
    "        exp[\"output_dir\"],\n",
    "        exp[\"per_device_batch_size\"],\n",
    "        exp[\"learning_rate\"],\n",
    "        exp[\"num_train_epochs\"],\n",
    "        exp[\"generation_max_length\"]\n",
    "    )\n",
    "    \n",
    "    # print params and the results\n",
    "    print(\"=== Results for experiment ===\")\n",
    "    print(\"-- Params --\") \n",
    "    print(json.dumps(exp, indent=4))\n",
    "    print(\"-- Eval results --\")\n",
    "    print(json.dumps(eval_results, indent=4))\n",
    "    \n",
    "    # save mapping between params and results\n",
    "    with open(exp[\"output_dir\"] + \"/params.json\", \"w\") as f:\n",
    "        json.dump(exp, f)\n",
    "    \n",
    "    with open(exp[\"output_dir\"] + \"/eval_results.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
